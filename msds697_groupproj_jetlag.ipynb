{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS697 Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Using SparkML to Predict New York City Taxi Fare --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group: Jetlag\n",
    "### Group Member: Jenny Kong, Joyce Chang, Xinke Sun, Zhe Yuan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pyspark_submit_args = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"myApp\").config(\"spark.mongodb.input.uri\",\\\n",
    "                                             \"mongodb://54.201.216.215/mydb.option\").getOrCreate()    \n",
    "# 54.201.216.215 -- mongos master public ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "raw = raw.drop(\"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning, Change DataType and Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_rdd = raw.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter fare which are less than 5.0 (initial charge)\n",
    "fare_rdd = raw_rdd.filter(lambda x: x[2] >= 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter location which are not belong to NYC\n",
    "location_rdd = fare_rdd.filter(lambda x: x[1]> -75 and x[1]< -73\n",
    "                              ).filter(lambda x: x[0]> 39 and x[0]< 43\n",
    "                                      ).filter(lambda x: x[7]> -75 and x[7]< -73\n",
    "                                              ).filter(lambda x: x[6]> 39 and x[6]< 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add 'time of day', drop original date\n",
    "# features : time, pickup_datetime, up_lat, up_lon, drop_lat, drop_lon, passenger_count, fare_amount\n",
    "final_rdd = location_rdd.map(lambda x: (x[3].split()[1].split(\".\")[0].split(\":\")[0],\n",
    "                                        x[5].split()[0], x[6], x[7], x[0], x[1], x[4],x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "def IntegerSafe(value): \n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        return None \n",
    "\n",
    "taxiSchema = StructType([StructField('hour_of_day', IntegerType(), False), \n",
    "                         StructField('pickup_datetime', StringType(), False),\n",
    "                         StructField('pickup_latitude', DoubleType(), False),\n",
    "                         StructField('pickup_longitude', DoubleType(), False), \n",
    "                         StructField('dropoff_latitude', DoubleType(), False), \n",
    "                         StructField('dropoff_longitude', DoubleType(), False),\n",
    "                         StructField('passenger_count', IntegerType(), False),\n",
    "                         StructField('fare_amount', DoubleType(), False)\n",
    "                         ])\n",
    "\n",
    "taxi_df = ss.createDataFrame(final_rdd.map(lambda x : (IntegerSafe(x[0]),x[1],x[2],\n",
    "                                                       x[3],x[4],x[5],x[6],x[7])),taxiSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------------+----------------+----------------+-----------------+---------------+-----------+\n",
      "|hour_of_day|pickup_datetime|pickup_latitude|pickup_longitude|dropoff_latitude|dropoff_longitude|passenger_count|fare_amount|\n",
      "+-----------+---------------+---------------+----------------+----------------+-----------------+---------------+-----------+\n",
      "|          4|     2012-04-21|      40.733143|       -73.98713|       40.758092|       -73.991567|              1|        7.7|\n",
      "|         20|     2012-11-20|      40.751662|      -73.980002|       40.764842|       -73.973802|              1|        7.5|\n",
      "|         13|     2012-12-03|      40.726713|      -74.006462|       40.731628|       -73.993078|              1|        9.0|\n",
      "|         19|     2013-07-02|      40.728867|       -74.00536|       40.710907|       -74.008913|              1|        7.0|\n",
      "|         17|     2011-04-05|      40.737547|      -74.001821|       40.722788|        -73.99806|              2|        7.7|\n",
      "+-----------+---------------+---------------+----------------+----------------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "str_date_func =  udf(lambda x: datetime.strptime(x, '%Y-%m-%d'), DateType())\n",
    "taxi_df = taxi_df.withColumn('date', str_date_func(taxi_df['pickup_datetime'])).drop(\"pickup_datetime\")\n",
    "taxi_df = taxi_df.withColumnRenamed('date', 'pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hour_of_day: integer (nullable = false)\n",
      " |-- pickup_latitude: double (nullable = false)\n",
      " |-- pickup_longitude: double (nullable = false)\n",
      " |-- dropoff_latitude: double (nullable = false)\n",
      " |-- dropoff_longitude: double (nullable = false)\n",
      " |-- passenger_count: integer (nullable = false)\n",
      " |-- fare_amount: double (nullable = false)\n",
      " |-- pickup_datetime: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create distance based on latitute and longitude\n",
    "from pyspark.sql.types import *\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "        dlon = lon2 - lon1 \n",
    "        dlat = lat2 - lat1 \n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371\n",
    "        return c * r * 1000\n",
    "    except AttributeError:\n",
    "        return None\n",
    "    \n",
    "haversine_udf = udf(lambda a, b, c, d: haversine(a,b,c,d),FloatType())\n",
    "dist_df = taxi_df.withColumn('dist', haversine_udf(taxi_df['pickup_longitude'], \n",
    "                              taxi_df['pickup_latitude'],\n",
    "                              taxi_df['dropoff_longitude'], \n",
    "                              taxi_df['dropoff_latitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create time related features, including year, month, quarter, day of month, day of week\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "year_mon_day_df = dist_df.withColumn('year', year(dist_df['pickup_datetime'])\n",
    "                                    ).withColumn('month', month(dist_df['pickup_datetime'])\n",
    "                                                ).withColumn('quarter', quarter(dist_df['pickup_datetime'])\n",
    "                                                            ).withColumn('day_of_month', dayofmonth(dist_df['pickup_datetime']))\n",
    "year_mon_day_df = year_mon_day_df.withColumn(\"day_of_week\", date_format(\"pickup_datetime\", \"u\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create rushhour feature and change data type\n",
    "rushhour_udf = udf(lambda x: 1 if (7 < x <10 or 16 < x < 19) else 0)\n",
    "rush_hour_df = year_mon_day_df.withColumn('rushhour', rushhour_udf('hour_of_day'))\n",
    "\n",
    "rush_hour_float_df = rush_hour_df.withColumn('rushhour_new', rush_hour_df['rushhour'].cast(FloatType())).drop(\"rushhour\")\n",
    "rush_hour_float_df = rush_hour_float_df.withColumnRenamed('rushhour_new', 'rushhour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rush_hour_float_c = rush_hour_float_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change data type : day_of_week\n",
    "day_week_float_df = rush_hour_float_c.withColumn('day_of_week_new', rush_hour_float_c['day_of_week'].cast(FloatType())).drop(\"day_of_week\")\n",
    "day_week_float_df = day_week_float_df.withColumnRenamed('day_of_week_new', 'day_of_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually one hot encoding for year, quarter, day_of_week\n",
    "def num1(value):\n",
    "    if value == 1: return 1\n",
    "    else: return 0\n",
    "def num2(value):\n",
    "    if value == 2: return 1\n",
    "    else: return 0\n",
    "def num3(value):\n",
    "    if value == 3: return 1\n",
    "    else: return 0\n",
    "def num4(value):\n",
    "    if value == 4: return 1\n",
    "    else: return 0\n",
    "def num5(value):\n",
    "    if value == 5: return 1\n",
    "    else: return 0\n",
    "def num6(value):\n",
    "    if value == 6: return 1\n",
    "    else: return 0\n",
    "def num7(value):\n",
    "    if value == 7: return 1\n",
    "    else: return 0\n",
    "\n",
    "num1_udf = udf(lambda x: num1(x))\n",
    "num2_udf = udf(lambda x: num2(x))\n",
    "num3_udf = udf(lambda x: num3(x))\n",
    "num4_udf = udf(lambda x: num4(x))\n",
    "num5_udf = udf(lambda x: num5(x))\n",
    "num6_udf = udf(lambda x: num6(x))\n",
    "num7_udf = udf(lambda x: num7(x))\n",
    "\n",
    "day_week_float_df = day_week_float_df.withColumn('day_1', num1_udf(day_week_float_df['day_of_week']))\n",
    "day_week_float_df = day_week_float_df.withColumn('day_2', num2_udf(day_week_float_df['day_of_week']))\n",
    "day_week_float_df = day_week_float_df.withColumn('day_3', num3_udf(day_week_float_df['day_of_week']))\n",
    "day_week_float_df = day_week_float_df.withColumn('day_4', num4_udf(day_week_float_df['day_of_week']))\n",
    "day_week_float_df = day_week_float_df.withColumn('day_5', num5_udf(day_week_float_df['day_of_week']))\n",
    "day_week_float_df = day_week_float_df.withColumn('day_6', num6_udf(day_week_float_df['day_of_week']))\n",
    "day_week_float_df = day_week_float_df.withColumn('day_7', num7_udf(day_week_float_df['day_of_week']))\n",
    "\n",
    "quarter_df = day_week_float_df.withColumn('quarter_1', num1_udf(day_week_float_df['quarter']))\n",
    "quarter_df = quarter_df.withColumn('quarter_2', num2_udf(quarter_df['quarter']))\n",
    "quarter_df = quarter_df.withColumn('quarter_3', num3_udf(quarter_df['quarter']))\n",
    "quarter_df = quarter_df.withColumn('quarter_4', num4_udf(quarter_df['quarter']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def year1(value):\n",
    "    if value == 2009: return 1\n",
    "    else: return 0\n",
    "\n",
    "def year2(value):\n",
    "    if value == 2010: return 1\n",
    "    else: return 0\n",
    "    \n",
    "def year3(value):\n",
    "    if value == 2011: return 1\n",
    "    else: return 0\n",
    "    \n",
    "def year4(value):\n",
    "    if value == 2012: return 1\n",
    "    else: return 0\n",
    "\n",
    "def year5(value):\n",
    "    if value == 2013: return 1\n",
    "    else: return 0\n",
    "    \n",
    "def year6(value):\n",
    "    if value == 2014: return 1\n",
    "    else: return 0\n",
    "    \n",
    "def year7(value):\n",
    "    if value == 2015: return 1\n",
    "    else: return 0\n",
    "\n",
    "\n",
    "year1_udf = udf(lambda x: year1(x))\n",
    "year2_udf = udf(lambda x: year2(x))\n",
    "year3_udf = udf(lambda x: year3(x))\n",
    "year4_udf = udf(lambda x: year4(x))\n",
    "year5_udf = udf(lambda x: year5(x))\n",
    "year6_udf = udf(lambda x: year6(x))\n",
    "year7_udf = udf(lambda x: year7(x))\n",
    "\n",
    "year_df = quarter_df.withColumn('year_1', year1_udf(quarter_df['year']))\n",
    "year_df = year_df.withColumn('year_2', year2_udf(quarter_df['year']))\n",
    "year_df = year_df.withColumn('year_3', year3_udf(quarter_df['year']))\n",
    "year_df = year_df.withColumn('year_4', year4_udf(quarter_df['year']))\n",
    "year_df = year_df.withColumn('year_5', year5_udf(quarter_df['year']))\n",
    "year_df = year_df.withColumn('year_6', year6_udf(quarter_df['year']))\n",
    "year_df = year_df.withColumn('year_7', year7_udf(quarter_df['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_df_c = year_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change data type for day_1~7\n",
    "day = [\"day_1\", \"day_2\", \"day_3\", \"day_4\", \"day_5\", \"day_6\", \"day_7\"]\n",
    "for d in day:\n",
    "    year_df_c = year_df_c.withColumn(d + \"_new\", year_df_c[d].cast(FloatType())).drop(d)\n",
    "    year_df_c = year_df_c.withColumnRenamed(d + \"_new\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_float_c = year_df_c.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change data type for quarter\n",
    "quarter = [\"quarter_1\", \"quarter_2\", \"quarter_3\", \"quarter_4\"]\n",
    "for q in quarter:\n",
    "    day_float_c = day_float_c.withColumn(q + \"_new\", day_float_c[q].cast(FloatType())).drop(q)\n",
    "    day_float_c = day_float_c.withColumnRenamed(d + \"_new\", q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quarter_float_c = day_float_c.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change data type for year\n",
    "year = [\"year_1\", \"year_2\", \"year_3\", \"year_4\", \"year_5\", \"year_6\", \"year_7\"]\n",
    "for y in year:\n",
    "    quarter_float_c = quarter_float_c.withColumn(y + \"_new\", quarter_float_c[y].cast(FloatType())).drop(y)\n",
    "    quarter_float_c = quarter_float_c.withColumnRenamed(y + \"_new\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_float_c = quarter_float_c.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "cols = list(set(year_float_c.columns) - {'fare_amount','quarter','dropoff_longitude','month',\\\n",
    "                                  'pickup_datetime','dropoff_latitude','pickup_latitude',\\\n",
    "                                  'day_of_week','year','pickup_longitude'})\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols = cols)\n",
    "va_df = va.transform(year_float_c).select(\"features\", \"fare_amount\").withColumnRenamed(\"fare_amount\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(23,[0,1,2,5,7,13...|  7.7|\n",
      "|(23,[0,1,2,7,15,1...|  7.5|\n",
      "|(23,[0,1,2,7,9,16...|  9.0|\n",
      "|(23,[0,1,2,4,12,1...|  7.0|\n",
      "|(23,[0,1,2,5,10,1...|  7.7|\n",
      "|(23,[0,1,2,4,15,1...| 10.5|\n",
      "|(23,[0,1,2,3,16,2...|  6.1|\n",
      "|(23,[0,1,2,5,10,1...|  7.3|\n",
      "|(23,[0,1,2,5,7,17...|  9.3|\n",
      "|(23,[0,1,2,5,6,18...|22.54|\n",
      "|(23,[0,1,2,14,17,...| 31.9|\n",
      "|(23,[0,1,2,4,8,10...| 18.1|\n",
      "|(23,[0,1,2,6,12,1...|  9.0|\n",
      "|(23,[0,1,2,9,14,2...|  9.8|\n",
      "|(23,[0,1,2,10,13,...| 10.9|\n",
      "|(23,[0,1,2,4,8,10...|  6.9|\n",
      "|(23,[0,1,2,12,14,...|  9.0|\n",
      "|(23,[0,1,2,10,15,...|  9.7|\n",
      "|(23,[0,1,2,4,12,1...|  7.5|\n",
      "|(23,[0,1,2,7,8,14...| 15.3|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splits = va_df.randomSplit([0.8, 0.2])\n",
    "train_df = splits[0].cache()\n",
    "test_df = splits[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Fit Multiple Regression Models, Do Transformation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.585697\n",
      "Root Mean Squared Error (RMSE) on test data = 6.42915\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='label', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "#fit model to train data\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "#do prediction on test dataset\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "\n",
    "#check prediction outcome\n",
    "lr_predictions.select(\"prediction\",\"label\",\"features\").show(5)\n",
    "\n",
    "#Evaluate the model\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\",metricName=\"r2\")\n",
    "\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n",
    "\n",
    "test_result = lr_model.evaluate(test_df) \n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.721088\n",
      "Root Mean Squared Error (RMSE) on test data = 5.89855\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\")\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_prediction = dt_model.transform(test_df)\n",
    "dt_prediction_c = dt_prediction.cache()\n",
    "\n",
    "dt_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"label\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % dt_evaluator.evaluate(dt_prediction))\n",
    "\n",
    "dt_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"label\",metricName=\"rmse\")\n",
    "print(â€œRoot Mean Squared Error (RMSE) on test data = %gâ€ % dt_evaluator.evaluate(dt_prediction_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.27507\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\")\n",
    "rf_model = rf.fit(train_df)\n",
    "rf_prediction = rf_model.transform(test_df)\n",
    "rf_prediction_c = rf_prediction.cache()\n",
    "\n",
    "rf_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"label\",metricName='rmse')\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rf_evaluator.evaluate(rf_prediction_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
